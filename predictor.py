# predictor.py
"""
äººæ•°é¢„æµ‹æ¨¡å—
------------
é¦–æ¬¡ import æ—¶ä¼šè‡ªåŠ¨æ£€æŸ¥å¹¶åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹ï¼›è‹¥æ¨¡å‹ä¸å­˜åœ¨ï¼Œå°†
åˆ©ç”¨ â€œé™„ä»¶1ï¼šä¸åŒäººæ•°çš„STRå›¾è°±æ•°æ®.xlsxâ€ é‡æ–°è®­ç»ƒå¹¶ä¿å­˜ã€‚

å¤–éƒ¨æ¥å£:
    predict(allele_list: list[str]) -> dict
        è¿”å›ç¤ºä¾‹: {"num": 3}
"""

import os
import re
import joblib
import numpy as np
import pandas as pd
from pathlib import Path
from scipy.stats import skew
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import (
    RandomForestClassifier, GradientBoostingClassifier,
    StackingClassifier, VotingClassifier
)
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
from imblearn.over_sampling import SMOTE
import xgboost as xgb
from lightgbm import LGBMClassifier

# ------------------- å¸¸é‡ -------------------
MODEL_PATH = Path(__file__).with_name("num_predictor.pkl")
DATA_PATH  = Path(__file__).with_name("é™„ä»¶1ï¼šä¸åŒäººæ•°çš„STRå›¾è°±æ•°æ®.xlsx")
MAX_MARKERS = 100      # ä¸åŸå§‹è„šæœ¬ä¿æŒä¸€è‡´

# ---------------- ç‰¹å¾å·¥ç¨‹å‡½æ•° ---------------
def extract_features(row: pd.Series) -> pd.Series:
    heights = [row.get(f"Height {i}") for i in range(1, MAX_MARKERS + 1)
               if pd.notna(row.get(f"Height {i}"))]
    alleles = [str(row.get(f"Allele {i}")).upper() for i in range(1, MAX_MARKERS + 1)
               if pd.notna(row.get(f"Allele {i}"))]
    sizes   = [row.get(f"Size {i}")   for i in range(1, MAX_MARKERS + 1)
               if pd.notna(row.get(f"Size {i}"))]

    if len(heights) < 2:
        return pd.Series([0] * 30)

    filtered_heights = [h for h in heights if h >= 9]
    sorted_heights   = sorted(heights, reverse=True)
    mean_h = np.mean(filtered_heights) if filtered_heights else 0
    allele_entropy = -sum(
        (alleles.count(a)/len(alleles)) *
        np.log2(alleles.count(a)/len(alleles)) for a in set(alleles)
    )

    return pd.Series([
        len(heights), np.max(heights), np.min(heights), mean_h, np.std(heights),
        np.std(heights)/mean_h if mean_h > 0 else 0,
        len(set(alleles)), alleles.count('OL'), sum(h < 50 for h in heights),
        np.max(heights)/np.min(heights) if np.min(heights) > 0 else 0,
        np.mean(sorted_heights[:3]), skew(heights) if len(set(heights)) > 1 else 0,
        (max(sizes)-min(sizes)) if sizes else 0, len(set(sizes)),
        sum(h > 70 for h in heights)/len(heights),
        sum(h > 100 for h in heights)/len(heights),
        np.median(heights), np.std(sizes) if sizes else 0,
        allele_entropy, sum(h < 10 for h in heights)/sum(heights) if sum(heights) > 0 else 0,
        sum(1 for h in heights if h < 50)/len(heights),
        sorted_heights[0] - sorted_heights[2] if len(sorted_heights) >= 3 else 0,
        sorted_heights[0] / (sorted_heights[1] + 1) if len(sorted_heights) >= 2 else 0,
        sum(a == 'OL' for a in alleles)/len(heights),
        np.median(sizes) if sizes else 0,
        len(set(sizes))/len(sizes) if sizes else 0,
        len(set(heights))/len(heights),
        np.var(heights), np.var(sizes) if sizes else 0,
        np.mean(sizes) if sizes else 0,
        len(alleles) / len(set(alleles)) if len(set(alleles)) > 0 else 0
    ])

def extract_advanced_features(row: pd.Series) -> pd.Series:
    heights = [row.get(f"Height {i}") for i in range(1, MAX_MARKERS + 1)
               if pd.notna(row.get(f"Height {i}"))]
    sizes   = [row.get(f"Size {i}")   for i in range(1, MAX_MARKERS + 1)
               if pd.notna(row.get(f"Size {i}"))]
    alleles = [str(row.get(f"Allele {i}")).upper() for i in range(1, MAX_MARKERS + 1)
               if pd.notna(row.get(f"Allele {i}"))]

    if len(heights) < 2 or len(sizes) < 2:
        return pd.Series([0] * 8)

    total_h  = sum(heights)
    top5_sum = sum(sorted(heights, reverse=True)[:5])
    height_size_ratios = [h/s for h, s in zip(heights, sizes) if s > 0]

    return pd.Series([
        top5_sum / total_h if total_h > 0 else 0,
        np.mean(height_size_ratios) if height_size_ratios else 0,
        np.std(height_size_ratios)  if len(height_size_ratios) > 1 else 0,
        np.var(sizes), np.std(sizes),
        skew(sizes) if len(set(sizes)) > 1 else 0,
        sum(h > 100 for h in heights) / len(alleles) if len(alleles) > 0 else 0,
        len([a for a in alleles if a != 'OL']) / len(set(alleles)) if len(set(alleles)) > 0 else 0
    ])

# ---------------- è®­ç»ƒæµç¨‹ -------------------
def _train_and_save():
    print("ğŸ”„ æ­£åœ¨è®­ç»ƒäººæ•°é¢„æµ‹æ¨¡å‹ ...")
    df = pd.read_excel(DATA_PATH, sheet_name="ä¸åŒäººæ•°çš„æ•°æ®é›†")

    # æå–çœŸå®äººæ•°æ ‡ç­¾
    df["Contributor_Count"] = df["Sample File"].apply(
        lambda x: len(re.search(r'RD14-0003-([\d_]+)-', str(x)).group(1).split('_'))
        if re.search(r'RD14-0003-([\d_]+)-', str(x)) else np.nan
    )

    # â¡ï¸ å¦‚éœ€åˆæˆ 5 äººæ ·æœ¬å¯æŒ‰åŸè„šæœ¬æ‹¼åˆï¼›æ­¤å¤„ä¿æŒç®€å•ï¼Œç›´æ¥ç”¨åŸå§‹æ•°æ®
    df = df.dropna(subset=["Contributor_Count"])

    # åŸºç¡€ + é«˜çº§ç‰¹å¾
    base_feat      = df.apply(extract_features, axis=1)
    advanced_feat  = df.apply(extract_advanced_features, axis=1)
    features_df    = pd.concat([base_feat, advanced_feat], axis=1)
    features_df.columns = [f"f{i}" for i in range(features_df.shape[1])]
    # å¡«è¡¥ç¼ºå¤±å€¼ï¼Œé˜²æ­¢ NaN ä¼ å…¥ SMOTE
    features_df = features_df.fillna(0)

    X = features_df
    y = df["Contributor_Count"].astype(int)

    # æ ‡ç­¾ç¼–ç 
    label_encoder = LabelEncoder()
    y_enc = label_encoder.fit_transform(y)

    # è¿‡é‡‡æ ·å¹³è¡¡
    X_res, y_res = SMOTE(random_state=42).fit_resample(X, y_enc)

    # -------- æ¨¡å‹é›†åˆ ----------
    rf  = RandomForestClassifier(n_estimators=200, max_depth=8, random_state=42)
    gb  = GradientBoostingClassifier(n_estimators=150, max_depth=6,
                                     learning_rate=0.08, random_state=42)
    xgb_model = xgb.XGBClassifier(n_estimators=150, max_depth=6,
                                  learning_rate=0.08, use_label_encoder=False,
                                  eval_metric='mlogloss', random_state=42)
    lgb_model = LGBMClassifier(n_estimators=120, max_depth=7,
                               learning_rate=0.07, random_state=42)
    lr  = LogisticRegression(max_iter=2000)

    stacking = StackingClassifier(
        estimators=[('rf', rf), ('gb', gb),
                    ('xgb', xgb_model), ('lgb', lgb_model)],
        final_estimator=lr, passthrough=True, cv=5, n_jobs=-1
    )
    voting = VotingClassifier(
        estimators=[('stacking', stacking), ('rf', rf), ('gb', gb),
                    ('xgb', xgb_model), ('lgb', lgb_model)],
        voting='soft', n_jobs=-1
    )

    X_tr, X_te, y_tr, y_te = train_test_split(
        X_res, y_res, test_size=0.2, stratify=y_res, random_state=42
    )
    voting.fit(X_tr, y_tr)

    # è®­ç»ƒé›†ç®€å•è¯„ä¼°
    y_pred = voting.predict(X_te)
    y_pred_dec = label_encoder.inverse_transform(y_pred)
    y_true_dec = label_encoder.inverse_transform(y_te)
    print(f"âœ… Accuracy: {accuracy_score(y_true_dec, y_pred_dec):.4f}")
    print(classification_report(y_true_dec, y_pred_dec))

    # æŒä¹…åŒ–
    joblib.dump({
        "model": voting,
        "label_encoder": label_encoder,
        "feature_cols": list(X.columns)
    }, MODEL_PATH)
    print(f"ğŸ‰ æ¨¡å‹å·²ä¿å­˜è‡³ {MODEL_PATH}")

# -------------- åŠ è½½æ¨¡å‹ ---------------------
def _load_model():
    if MODEL_PATH.exists():
        return joblib.load(MODEL_PATH)
    if not DATA_PATH.exists():
        raise FileNotFoundError(
            f"æ‰¾ä¸åˆ°è®­ç»ƒæ•°æ® {DATA_PATH}ï¼Œæ— æ³•è®­ç»ƒæ¨¡å‹ï¼"
        )
    _train_and_save()
    return joblib.load(MODEL_PATH)

_model_bundle = _load_model()           # å…¨å±€åŠ è½½
_model        = _model_bundle["model"]
_label_enc    = _model_bundle["label_encoder"]
_feat_cols    = _model_bundle["feature_cols"]

# -------------- é¢„æµ‹æ¥å£ ---------------------
def _build_row_series(flat_list):
    """
    å°† [allele1,size1,height1, allele2,size2,height2, ...]
    è½¬æ¢ä¸ºä¸è®­ç»ƒé˜¶æ®µä¸€è‡´çš„â€œè¡Œè®°å½•â€ (pandas.Series)
    """
    if len(flat_list) % 3 != 0:
        raise ValueError("è¾“å…¥åˆ—è¡¨é•¿åº¦å¿…é¡»æ˜¯ 3 çš„å€æ•° (allele, size, height ä¸‰å…ƒç´ ä¸€ç»„)")

    row_dict = {}
    triplets = [flat_list[i:i+3] for i in range(0, len(flat_list), 3)]
    for idx, (allele, size, height) in enumerate(triplets, start=1):
        if idx > MAX_MARKERS:
            break
        # Alleleï¼šä¿ç•™åŸå§‹å­—ç¬¦ä¸² (å¯èƒ½å« 'OL')
        row_dict[f"Allele {idx}"] = str(allele).strip()
        # Size / Heightï¼šè½¬ floatï¼Œéæ³•å€¼è®¾ NaN
        try:
            row_dict[f"Size {idx}"] = float(size)
        except ValueError:
            row_dict[f"Size {idx}"] = np.nan
        try:
            row_dict[f"Height {idx}"] = float(height)
        except ValueError:
            row_dict[f"Height {idx}"] = np.nan

    # ç¡®ä¿æ‰€æœ‰é”®é½å…¨ï¼ˆç¼ºå¤±éƒ¨åˆ†å¡« NaNï¼‰
    for i in range(1, MAX_MARKERS + 1):
        for col in ("Allele", "Size", "Height"):
            row_dict.setdefault(f"{col} {i}", np.nan)

    return pd.Series(row_dict)

def predict(allele_list):
    """
    é¢„æµ‹è´¡çŒ®è€…äººæ•°

    å‚æ•°
    ----
    allele_list : list[str]
        å½¢å¦‚ ["allele1","size1","height1","allele2",...]
        (é•¿åº¦å¿…é¡»æ˜¯ 3 çš„å€æ•°)

    è¿”å›
    ----
    dict : {"num": int}
    """
    series_row = _build_row_series(allele_list)

    base_feat     = extract_features(series_row)
    advanced_feat = extract_advanced_features(series_row)
    feat_row      = pd.concat([base_feat, advanced_feat])
    feat_row.index = _feat_cols            # ä¿è¯åˆ—é¡ºåºä¸€è‡´
    feat_df       = pd.DataFrame([feat_row])

    pred_enc  = _model.predict(feat_df)[0]
    pred_num  = int(_label_enc.inverse_transform([pred_enc])[0])
    return {"num": pred_num}

# ---------------- CLI æ–¹ä¾¿è°ƒè¯• ---------------
if __name__ == "__main__":
    # ç¤ºä¾‹ï¼š12,104,1320,14,108,980,13,110,780
    test = input("è¯·è¾“å…¥ allele,size,height é€—å·åˆ†éš”åˆ—è¡¨ï¼š\n")
    flat = [s.strip() for s in test.split(",") if s.strip()]
    print("é¢„æµ‹ç»“æœ:", predict(flat))